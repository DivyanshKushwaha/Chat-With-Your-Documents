# ğŸ“„ RAG Chatbot with Pinecone and Gemini AI

This project is a **Retrieval-Augmented Generation (RAG) chatbot** that allows users to upload documents (PDF, DOCX, TXT) and chat with them using **Google Gemini AI**. The documents are stored as vector embeddings in **Pinecone**, enabling efficient retrieval of relevant information when a query is made.

## ğŸš€ Features
- **Upload & Process Documents**: Supports PDF, TXT, and DOCX files.
- **Embeddings & Storage**: Uses **Google Gemini** for embeddings and **Pinecone** as a vector database.
- **Efficient Retrieval**: Fetches the most relevant text chunks from documents using vector similarity search.
- **Chat with Documents**: Uses **Google Gemini AI** to generate responses based on retrieved text.

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/DivyanshKushwaha/Chat-With-Your-Documents.git
cd Chat-With-Your-Documents
```
### 2ï¸âƒ£ Create & Activate Virtual Environment
- On Windows (PowerShell)
    ```bash
    python -m venv venv
    venv\Scripts\Activate
    ```

### 3ï¸âƒ£ Install Dependencies
- Give command in powershell 
    ```bash
    pip install -r requirements.txt
    ```
- If <b>pinecone[grpc]</b> module gives error while installing, Give this command 
    ```bash 
    pip install "pinecone[grpc]"
    ```


## ğŸ”‘ API Keys Setup

### ğŸŒ² Pinecone Setup
- Go to Pinecone and sign up/log in.
- Create a new index with: 
    - Index Name: your_index_name
    - Dimension: 768 (for Gemini embeddings)
    - Metric: cosine
- Copy your Pinecone API key from the dashboard.
- Change 'INDEX_NAME' value in db.py to 'your_index_name'
    ```bash 
    INDEX_NAME = "your_index_name"
    ```


### ğŸ¤– Google Gemini AI Setup
Go to Google AI Studio.
Generate an API Key for Gemini.

## ğŸ“‚ Environment Variables
- Create a .env file in the project root and add
    ```bash
    PINECONE_API_KEY="your-pinecone-api-key"
    GEMINI_API_KEY="your-gemini-api-key"
    ```
## â–¶ï¸ Running the Application
- Once everything is set up, launch the application 
    ```bash 
    streamlit run app.py 
    ```
- Access application on 
    ```bash 
    http://localhost:8501
    ```

## ğŸ“œ File Structure
- After running and uploading a document you will see structure like 
    ```bash 
    ğŸ“‚ Chat-With-Your-Documents
    â”œâ”€â”€ ğŸ“‚ data
    â”‚   â”œâ”€â”€ ğŸ“‚ uploads  # Uploaded documents
    â”œâ”€â”€ ğŸ“‚ utils
    â”‚   â”œâ”€â”€ db.py       # Pinecone VectorStore Management
    â”‚   â”œâ”€â”€ file_handler.py # Handles file uploads & text extraction
    â”‚   â”œâ”€â”€ llm.py      # Google Gemini AI LLM integration
    â”‚   â”œâ”€â”€ rag.py      # RAG pipeline (retrieval + generation)
    â”œâ”€â”€ app.py          # Streamlit frontend
    â”œâ”€â”€ requirements.txt # Dependencies
    â”œâ”€â”€ .env            # API Keys (not committed)
    â””â”€â”€ README.md       # Project Documentation
    ```


## ğŸ› ï¸ How It Works
- Upload a document â†’ The file is processed and embeddings are stored in Pinecone.
- Query the chatbot â†’ Retrieves relevant text from Pinecone and passes it to Gemini AI.
- Generate a response â†’ Gemini AI uses the retrieved context to answer your query.


## ğŸ† Optimizations Implemented
- âœ… Avoid duplicate embeddings:
        - The app checks if a document is already stored in Pinecone before adding it again, saving costs.
- âœ… Efficient querying:
        - Uses namespaces in Pinecone to organize documents.
        - Limits unnecessary API calls to Google Gemini.
- âœ… Streamlit UI improvements:
        - Prevents auto-triggered responses when typing.
